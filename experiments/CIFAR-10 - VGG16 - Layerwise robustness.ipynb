{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise Pruning Robustness\n",
    "Prune each layer removing units iteratively according to the ranking of different attribution metrics. Plot the test loss and accuracy against the number of units that have been removed at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"./..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary, torchsummary\n",
    "from torchpruner import (Pruner, ShapleyAttributionMetric)\n",
    "from experiments.utils import get_module_name, format_plt, map_method_vis\n",
    "import experiments.models.cifar10 as cifar10\n",
    "from experiments.train import train, test\n",
    "\n",
    "from torchpruner import (\n",
    "    WeightNormAttributionMetric,\n",
    "    RandomAttributionMetric,\n",
    "    SensitivityAttributionMetric,\n",
    "    TaylorAttributionMetric,\n",
    "    APoZAttributionMetric,\n",
    "    ShapleyAttributionMetric\n",
    ")\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print (f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-13 14:59:38--  https://drive.google.com/u/0/uc?id=164ZfuLwocQRsnXPXKHr6Gbi5Pu7Ikihn&export=download\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.168.14, 2a00:1450:400a:801::200e\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.168.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0o-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/us78o36rkad95h1qikmb4hggotr4h0ek/1581601500000/04535366121136529507/*/164ZfuLwocQRsnXPXKHr6Gbi5Pu7Ikihn?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-02-13 14:59:41--  https://doc-0o-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/us78o36rkad95h1qikmb4hggotr4h0ek/1581601500000/04535366121136529507/*/164ZfuLwocQRsnXPXKHr6Gbi5Pu7Ikihn?e=download\n",
      "Resolving doc-0o-7s-docs.googleusercontent.com (doc-0o-7s-docs.googleusercontent.com)... 172.217.168.33, 2a00:1450:400a:802::2001\n",
      "Connecting to doc-0o-7s-docs.googleusercontent.com (doc-0o-7s-docs.googleusercontent.com)|172.217.168.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘weights/CIFAR10-VGG16.pt’\n",
      "\n",
      "weights/CIFAR10-VGG     [    <=>             ]  58.23M  92.0MB/s    in 0.6s    \n",
      "\n",
      "2020-02-13 14:59:42 (92.0 MB/s) - ‘weights/CIFAR10-VGG16.pt’ saved [61063971]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained weights\n",
    "!wget -O weights/CIFAR10-VGG16.pt \"https://drive.google.com/u/0/uc?id=164ZfuLwocQRsnXPXKHr6Gbi5Pu7Ikihn&export=download\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "             ReLU-10          [-1, 128, 16, 16]               0\n",
      "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
      "             ReLU-13          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
      "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
      "             ReLU-17            [-1, 256, 8, 8]               0\n",
      "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
      "             ReLU-20            [-1, 256, 8, 8]               0\n",
      "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
      "             ReLU-23            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
      "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-27            [-1, 512, 4, 4]               0\n",
      "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-30            [-1, 512, 4, 4]               0\n",
      "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-33            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
      "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-37            [-1, 512, 2, 2]               0\n",
      "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-40            [-1, 512, 2, 2]               0\n",
      "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-43            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
      "          Dropout-45                  [-1, 512]               0\n",
      "           Linear-46                  [-1, 512]         262,656\n",
      "             ReLU-47                  [-1, 512]               0\n",
      "          Dropout-48                  [-1, 512]               0\n",
      "           Linear-49                  [-1, 512]         262,656\n",
      "             ReLU-50                  [-1, 512]               0\n",
      "           Linear-51                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 15,253,578\n",
      "Trainable params: 15,253,578\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.59\n",
      "Params size (MB): 58.19\n",
      "Estimated Total Size (MB): 64.79\n",
      "----------------------------------------------------------------\n",
      "Test set: Average loss: 0.3827, Accuracy: 925/1000 (92.500%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = cifar10.get_dataset()\n",
    "# Using only 1000 samples from test set, just to speed up things a bit.\n",
    "# Alternatively, this test could be run faster using a forward_partial function\n",
    "test_set, _ = torch.utils.data.random_split(test_set, [1000, len(test_set)-1000])\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=1000, shuffle=False,\n",
    ")\n",
    "x_test, y_test = next(iter(test_loader))\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "# Use part of the train data to compute attributions\n",
    "train_set, _ = torch.utils.data.random_split(train_set, [1000, len(train_set)-1000])\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=100, shuffle=False,\n",
    ")\n",
    "    \n",
    "loss = cifar10.loss\n",
    "input_size = (3, 32, 32)\n",
    "\n",
    "# Load pretrained model\n",
    "def load_model():\n",
    "    model, name = cifar10.get_vgg_model_with_name()\n",
    "    # Load weights\n",
    "    model.load_state_dict(torch.load(\"weights/CIFAR10-VGG16.pt\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Display architecture and test performance\n",
    "model = load_model()\n",
    "summary(model, input_size=input_size, device=device.type)\n",
    "original_loss, original_acc = test(model, device, loss, test_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a function that returns the pruning graph for a VGG network\n",
    "# The pruning graph is a list of tuples (module, [cascading_modules])\n",
    "# where module is a prunable 'module' and cascading_modules contains all\n",
    "# the modules that should be pruned in consequence of 'module' being pruned\n",
    "def get_pruning_graph(vgg):\n",
    "    modules = list(vgg.features.children()) + list(vgg.classifier.children())\n",
    "    pruning = []\n",
    "    current = None\n",
    "\n",
    "    for module in modules:\n",
    "        if any([isinstance(module, c) for c in [nn.Linear, nn.Conv2d]]):\n",
    "            if current is not None:\n",
    "                pruning[-1][1].append(module)\n",
    "                pruning[-1][1].reverse()\n",
    "            current = module\n",
    "            pruning.append((module, []))\n",
    "        elif (\n",
    "            any([isinstance(module, c) for c in [nn.BatchNorm2d, nn.Dropout]])\n",
    "            and current is not None\n",
    "        ):\n",
    "            pruning[-1][1].append(module)\n",
    "    return pruning[::-1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize attribution metrics that will be used\n",
    "methods = {\n",
    "    \"Weight Norm\" : WeightNormAttributionMetric(model, val_loader, loss, device),\n",
    "    \"Random\" : RandomAttributionMetric(model, val_loader, loss, device),\n",
    "    \"Sensitivity\" : SensitivityAttributionMetric(model, val_loader, loss, device),\n",
    "    \"Taylor\" : TaylorAttributionMetric(model, val_loader, loss, device),\n",
    "    \"APoZ\" : APoZAttributionMetric(model, val_loader, loss, device),\n",
    "    \"SV\" : ShapleyAttributionMetric(model, val_loader, loss, device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test, for all layers and all attribution methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 15 modules...\n",
      "Running on classifier.4\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on classifier.1\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.40\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.37\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.34\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.30\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.27\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.24\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.20\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.17\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n",
      "\t--> SV (run 2)\n",
      "\t--> SV (run 3)\n",
      "Running on features.14\n",
      "\t--> Weight Norm (run 1)\n",
      "\t--> Random (run 1)\n",
      "\t--> Random (run 2)\n",
      "\t--> Random (run 3)\n",
      "\t--> Sensitivity (run 1)\n",
      "\t--> Taylor (run 1)\n",
      "\t--> APoZ (run 1)\n",
      "\t--> SV (run 1)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pruning_graph = get_pruning_graph(model)\n",
    "print (f\"Pruning {len(pruning_graph)} modules...\")\n",
    "\n",
    "log = {}\n",
    "for module, cascading_modules in pruning_graph:\n",
    "    module_name = get_module_name(model, module)\n",
    "    log[module_name] = {}\n",
    "    print (f\"Running on {module_name}\")\n",
    "    \n",
    "    for method_name, attribution in methods.items():\n",
    "        log[module_name][method_name] = {\"loss\" : [], \"acc\" : []}\n",
    "        pruned_indices = []\n",
    "        runs = 3 if method_name in [\"Random\", \"SV\"] else 1\n",
    "        \n",
    "        for r in range(runs):\n",
    "            print (f\"\\t--> {method_name} (run {r+1})\")\n",
    "            log[module_name][method_name][\"loss\"].append([original_loss])\n",
    "            log[module_name][method_name][\"acc\"].append([original_acc])\n",
    "\n",
    "            # Compute attributions on a subset of the train data\n",
    "            scores, ranking = attribution.run([module])[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Start ablation procedure, measuring performance on the test data\n",
    "                # We use simulated pruning (activations masked to zero)\n",
    "                \n",
    "                # 1. Compute activation of current module/layer.\n",
    "                z = model.forward_partial(x_test, to_module=module)\n",
    "\n",
    "                # 2. Remove activations sequentially, starting with thos having lowest attributions\n",
    "                for i in ranking:\n",
    "                    z.index_fill_(1, torch.tensor([i]).to(device), 0.)\n",
    "                    y_pred = model.forward_partial(z, from_module=module)\n",
    "                    new_loss = loss(y_pred, y_test)\n",
    "                    current_pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "                    new_acc = current_pred.eq(y_test.view_as(current_pred)).sum().float() / len(y_test)\n",
    "                    log[module_name][method_name][\"loss\"][-1].append(new_loss.item())\n",
    "                    log[module_name][method_name][\"acc\"][-1].append(new_acc.item())\n",
    "\n",
    "# Dump all results to file for later analysis\n",
    "pickle.dump(log, open(\"layerwise_pruning.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from previously stored results\n",
    "# and sort layers\n",
    "plot_data = pickle.load(open(\"layerwise_pruning.p\", \"rb\" ))\n",
    "layers = plot_data.keys()\n",
    "layers = sorted(layers, key=lambda x: -int(x.split(\".\")[1]), reverse=True)\n",
    "layers = sorted(layers, key=lambda x: 0 if \"classifier\" in x else 1, reverse=True)\n",
    "\n",
    "# Some general settings\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "for idx, layer_id in enumerate(layers):\n",
    "    layer_data = plot_data[layer_id]\n",
    "    layer_name = f\"{'Conv ' if 'features' in layer_id else 'Dense '} {idx}\"\n",
    "    fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(10,3.0));\n",
    "    \n",
    "    for method_name, data in layer_data.items(): \n",
    "        label, color = map_method_vis(method_name)\n",
    "        ticks = range(len(data[\"loss\"][0]))\n",
    "        loss_mean = np.mean(data[\"loss\"], 0)\n",
    "        loss_std = np.std(data[\"loss\"], 0)\n",
    "        acc_mean = np.mean(data[\"acc\"], 0)\n",
    "        acc_std = np.std(data[\"acc\"], 0)\n",
    "        \n",
    "        # Plot with confidence intervals\n",
    "        ax_loss.fill_between(ticks, loss_mean + loss_std, loss_mean - loss_std, alpha=.2, color=color);\n",
    "        ax_loss.plot(ticks, loss_mean, label=label, color=color, lw=1);\n",
    "        format_plt(ax_loss, f\"{layer_name} - Loss\", \"# units removed\", \"Avg. Test Loss\")\n",
    "\n",
    "        ax_acc.fill_between(ticks, acc_mean + acc_std, acc_mean - acc_std, alpha=.2, color=color);\n",
    "        ax_acc.plot(ticks, acc_mean, label=label, color=color, lw=1);\n",
    "        format_plt(ax_acc, f\"{layer_name} - Accuracy\", \"# units removed\", \"Test Accuracy (%)\")\n",
    "        # plt.savefig(f\"cifar10_robustness_study_{layer_name}.pdf\", bbox_inches='tight')\n",
    "        \n",
    "\n",
    "# Plot a single legend below\n",
    "handles, labels = ax_loss.get_legend_handles_labels()\n",
    "handles, labels  = zip(*sorted(zip(handles, labels), key=lambda t: 1 if \"SV\" in t[1] else -1))\n",
    "fig = plt.figure(figsize=(10, 1))\n",
    "fig.add_subplot(111)\n",
    "plt.box(False)\n",
    "plt.axis('off')\n",
    "fig.legend(handles, labels,loc='upper center', ncol=4);\n",
    "# plt.savefig(f\"cifar10_robustness_study_legend.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
